{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mecanismoMLRedWifi.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YeisonHunt/RL_algorithm_tesis/blob/master/mecanismoMLRedWifi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIFIIsbDY-5V"
      },
      "outputs": [],
      "source": [
        "#===============================================================#\n",
        "# LIBRERIAS\n",
        "import numpy as np\n",
        "from numpy.random.mtrand import random_integers\n",
        "\n",
        "#print(\"== Libraries OK! ==\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#===============================================================#\n",
        "#                         MLActuator                            #\n",
        "#===============================================================#\n",
        "class MLActuator:\n",
        "\n",
        "  #=== Constructor\n",
        "  def __init__(self, dicStates, dicActions):\n",
        "\n",
        "    #Estados\n",
        "    self.dicStates = dicStates\n",
        "    self.old_state = \"00\"\n",
        "    self.state = \"00\"\n",
        "\n",
        "    #Acciones\n",
        "    self.dicActions = dicActions\n",
        "    self.lastAction = \"00\"\n",
        "\n",
        "    print(\"MLActuator Created!\")\n",
        "\n",
        "  def get_lastAction(self):\n",
        "    return self.lastAction\n",
        "\n",
        "  def get_state_key(self):\n",
        "    return self.state\n",
        "\n",
        "  def get_old_state_key(self):\n",
        "    return self.old_state\n",
        "\n",
        "  def get_old_state_value(self):\n",
        "    if self.old_state in self.dicStates:\n",
        "      return self.dicStates[self.old_state]\n",
        "    else:\n",
        "      return \"None\"\n",
        "\n",
        "  #=== Step: Ejecuta la acción\n",
        "  def step(self, action):\n",
        "\n",
        "    print(\"==\")\n",
        "    print(\"Actuator Step\")\n",
        "\n",
        "    if action in self.dicActions:\n",
        "\n",
        "      #Actualizar estado anterior\n",
        "      self.old_state = self.state\n",
        "\n",
        "      #Obtener estado según la acción\n",
        "      self.state = action\n",
        "\n",
        "      #print(\"XXX old_state: \" + self.dicStates[self.old_state])\n",
        "\n",
        "      #### AQUI LLAMAR FUNCIONES DE TRASPASO (CODIGO ANDRES) ###\n",
        "\n",
        "      #Mostrar estado anterior\n",
        "      if self.old_state in self.dicStates:\n",
        "        print(\"old_state: \" + self.dicStates[self.old_state])\n",
        "\n",
        "      #Mostrar estado nuevo\n",
        "      print(\"state: \" + self.dicStates[self.state])\n",
        "\n",
        "      #Guardar última acción ejecutada\n",
        "      self.lastAction = action\n",
        "\n",
        "    else:\n",
        "      print(\"Acción no valida!!!\")\n",
        "\n",
        "    print(\"End Actuator Step!\")\n",
        "    print(\"\")\n",
        "\n",
        "#print(\"== MLActuator OK! ==\")"
      ],
      "metadata": {
        "id": "TsJBkBqEGkvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#===============================================================#\n",
        "#                           MLAgent                             #\n",
        "#===============================================================#\n",
        "\n",
        "class MLAgent:\n",
        "\n",
        "  #=== Constructor\n",
        "  def __init__(self, dicStates, dicActions, discount_factor, learning_rate, ratio_explotacion):\n",
        "    #Variables definidas por desarrollador\n",
        "    self.dicStates = dicStates\n",
        "    self.dicActions = dicActions\n",
        "\n",
        "    self.nStates = len(dicStates)\n",
        "    self.nActions = len(dicActions)\n",
        "\n",
        "    #Ecuación de Bellman\n",
        "    self.discount_factor = discount_factor  #L\n",
        "    self.learning_rate = learning_rate      #a\n",
        "    self.ratio_explotacion = ratio_explotacion\n",
        "\n",
        "    #POLITICAS (Q-TABLE)\n",
        "    self.q_table = np.zeros([self.nStates, self.nActions])\n",
        "\n",
        "    print(\"MLAgent Created!\")\n",
        "\n",
        "  #==Sección Bellman\n",
        "  def setQParameters(self, discount_factor, learning_rate, ratio_explotacion):\n",
        "    self.discount_factor = discount_factor  #L\n",
        "    self.learning_rate = learning_rate      #a\n",
        "    self.ratio_explotacion = ratio_explotacion\n",
        "\n",
        "  #=== Método para tomar el siguiente paso\n",
        "  def get_next_step(self, state, nAccessPoint, nSlices):\n",
        "    \n",
        "    #Variable para determinar si el aprendizaje se completa\n",
        "    q_table_completed = True\n",
        "\n",
        "    print(\"==\")\n",
        "    print(\"Agent Step\")\n",
        "\n",
        "    #Elegir si explotar o explorar para decidir si seguir aprendiendo o elegir la mejor opción\n",
        "    explotar = np.random.uniform()\n",
        "    if explotar<=self.ratio_explotacion:\n",
        "      elegir_mejor_opcion = False\n",
        "      print(\"Explotar    : \" + str(explotar) + \"<=\" + str(self.ratio_explotacion))\n",
        "    else:\n",
        "      elegir_mejor_opcion = True\n",
        "      print(\"NO Explotar : \" + str(explotar) + \">\" + str(self.ratio_explotacion))\n",
        "\n",
        "    #NOTA: Pero si existe un mejor paso entonces dar el mejor paso (Mejor acción)\n",
        "    posible_next_stepi = 0\n",
        "    if elegir_mejor_opcion:\n",
        "      maxQ = -1\n",
        "      posible_next_stepi = -1\n",
        "      statei = list(self.dicActions.keys()).index(state)\n",
        "\n",
        "      for ai in range(0, self.nActions):\n",
        "        q_value = self.q_table[int(statei),ai]\n",
        "        if(q_value>maxQ):\n",
        "          maxQ = q_value\n",
        "          posible_next_stepi = int(ai)\n",
        "\n",
        "      if(posible_next_stepi>=0):\n",
        "        posible_next_step = list(self.dicActions.keys())[posible_next_stepi]\n",
        "        #next_step = posible_next_step;\n",
        "        print(\"Mejor Acción (Posible): \" + self.dicActions[posible_next_step])\n",
        "    else:\n",
        "      ################################################\n",
        "      #Dar un paso aleatorio (Acción aleatoria): \n",
        "      #- Definido por el desarrollo: Conectarse a Slice_jk Aleatoriamente\n",
        "      posible_next_stepi = random_integers(0, self.nActions-1)\n",
        "      posible_next_step = list(self.dicActions.keys())[posible_next_stepi]\n",
        "      print(\"Acción Aleatoria (Posible): \" + self.dicActions[posible_next_step])\n",
        "      ################################################\n",
        "\n",
        "    next_step = posible_next_step\n",
        "\n",
        "    #Mostrar paso siguiente seleccionado\n",
        "    print(\"Acción Seleccionada : \" + self.dicActions[next_step])\n",
        "    print(\"End Agent Step!\")\n",
        "    print(\"==\")\n",
        "\n",
        "    return next_step\n",
        "\n",
        "  #=== Método para actualizar las políticas con las recompensas obtenidas\n",
        "  def update(self, old_state, action_taken, reward_action_taken):\n",
        "\n",
        "    print(\"==\")\n",
        "    print(\"Agent Update!\")\n",
        "\n",
        "    if old_state in self.dicStates:\n",
        "      print(\"oldstate: \" + self.dicStates[old_state])\n",
        "    \n",
        "    if action_taken in self.dicActions:\n",
        "      print(\"actiontk: \" + self.dicActions[action_taken])\n",
        "\n",
        "    #obtener índice correspondiente al estado en la Q-Table\n",
        "    idx_old_state = int(list(self.dicStates.keys()).index(old_state))\n",
        "\n",
        "    #obtener índice correspondiente de la acción en la Q-Table\n",
        "    idx_action_taken = int(list(self.dicActions.keys()).index(action_taken))\n",
        "\n",
        "    #### AQUI ACTUALIZAR Q-TABLE ###\n",
        "    ### Aplicar ecuación de Bellman ###\n",
        "\n",
        "    #Obtener parámetros de la ecuación de Bellman\n",
        "    Qsa = self.q_table[idx_old_state,idx_action_taken] #Q(S,A)\n",
        "\n",
        "    L = self.discount_factor     #Tasa de descuento\n",
        "    a = self.learning_rate       #Radio de aprendizaje\n",
        "    R = reward_action_taken #Recompensa\n",
        "\n",
        "    #maxQ = max(self.q_table[idx_old_state:]).any()\n",
        "    #print(\"MaxQ =\" + str(maxQ))\n",
        "\n",
        "    maxQ = 0\n",
        "    for ai in range(0,self.nActions):\n",
        "      q_value = self.q_table[idx_old_state, ai]\n",
        "      if(q_value>maxQ):\n",
        "        maxQ = q_value\n",
        "\n",
        "    print(\"MaxQ =\" + str(maxQ))\n",
        "\n",
        "    self.q_table[idx_old_state,idx_action_taken] = Qsa + a*(R+(L*maxQ)-Qsa)\n",
        "\n",
        "    print(\"End Agent Update!\")\n",
        "    print(\"==\")\n",
        "\n",
        "\n",
        "  #=== Método para mostrar la tabla de políticas (Q-Table)\n",
        "  def showQ(self):\n",
        "    print(\"Q-TABLE\")\n",
        "    print(self.q_table)\n",
        "    print(\"#===============================================================#\")\n",
        "\n",
        "#print(\"== MLAgent OK! ==\")\n"
      ],
      "metadata": {
        "id": "eWQf-qBkIDew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#===============================================================#\n",
        "#                      MechanismBasedOnML                       #\n",
        "#===============================================================#\n",
        "class MechanismBasedOnML:\n",
        "\n",
        "  #=== Constructor\n",
        "  def __init__(self, nAccessPoint, nSlices, discount_factor, learning_rate, ratio_explotacion):\n",
        "\n",
        "    # Datos de la red WiFi\n",
        "    self.nAccessPoint = nAccessPoint\n",
        "    self.nSlices = nSlices\n",
        "    \n",
        "    # Posibles Estados Estático\n",
        "    self.dicStates = {\"00\":\"NO conectado\"}\n",
        "    for ai in range(1,self.nAccessPoint+1):\n",
        "      for si in range(0,self.nSlices):\n",
        "        #key = str(ai) + str(si)  #FORMATO STRING\n",
        "        key = ai*10 + si          #FORMATO INT\n",
        "        self.dicStates.update({key :\"Conectado(\"+ str(ai) + \",\" + str(si)+\")\"})\n",
        "\n",
        "    # Posibles Acciones Estático\n",
        "    self.dicActions = {\"00\":\"Desconectarse\"}\n",
        "    for ai in range(1,self.nAccessPoint+1):\n",
        "      for si in range(0,self.nSlices):\n",
        "        #key = str(ai) + str(si)  #FORMATO STRING\n",
        "        key = ai*10 + si          #FORMATO INT\n",
        "        self.dicActions.update({key :\"Conectar_a(\"+ str(ai) + \",\" + str(si)+\")\"})\n",
        "\n",
        "    #Agente\n",
        "    self.agent = MLAgent(self.dicStates, self.dicActions, discount_factor, learning_rate, ratio_explotacion)\n",
        "\n",
        "    #Actuador\n",
        "    self.actuator = MLActuator(self.dicStates, self.dicActions)\n",
        "    \n",
        "    #Recompensa/Penalización\n",
        "    self.reward = 0\n",
        "\n",
        "    #Variables auxiliares\n",
        "    self.old_state = \"00\"\n",
        "\n",
        "    print(\"MechanismBasedOnML Created!\")\n",
        "    \n",
        "  #=== Step: Ejecuta la acción\n",
        "  def step(self):\n",
        "    self.old_state = self.actuator.get_state_key()\n",
        "    new_action = self.agent.get_next_step(self.old_state, self.nAccessPoint, self.nSlices)\n",
        "    self.actuator.step(new_action)\n",
        "\n",
        "  #=== Update: Actualiza las políticas\n",
        "  def update(self, new_action, reward):\n",
        "    self.agent.update(self.old_state, new_action, reward)\n",
        "    self.agent.showQ()\n",
        "\n",
        "#print(\"== MechanismBasedOnML OK! ==\")\n",
        "\n"
      ],
      "metadata": {
        "id": "cgHdZ-mAx-qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#===============================================================#\n",
        "#                        SDNController                          #\n",
        "#===============================================================#\n",
        "class SDNController:\n",
        "\n",
        "  #=== Constructor\n",
        "  def __init__(self):\n",
        "    self.state = \"00\"\n",
        "    self.reward = 0\n",
        "    print(\"SDNController Created!\")\n",
        "\n",
        "\n",
        "  #=== Asignar estado\n",
        "  def set_State(self, newState):\n",
        "    self.state = newState\n",
        "\n",
        "  #=== Calcular recompensa\n",
        "  def calcularReward(self):\n",
        "    # Calcular reward según las variables\n",
        "    self.reward = 1\n",
        "\n",
        "  #=== Monitoreo\n",
        "  def monitoring(self, j_k_list, action):\n",
        "\n",
        "    #Procesar aquí los parámetros para obtener el nuevo estado y la recompensa\n",
        "\n",
        "    #Validar si el estado=acción está en la lista de Slice que el carro escucha\n",
        "    if action in j_k_list.keys():\n",
        "\n",
        "      print()\n",
        "      print(\"j_k_list \" + str(action))\n",
        "      print(j_k_list[action])\n",
        "      print(\"================\")    \n",
        "\n",
        "\n",
        "      #== Orden de datos sel Slice_j_k\n",
        "      # slice_id\n",
        "      # target_mac_j_k\n",
        "      # latency_server\n",
        "      # load_j_k_t_1\n",
        "      # load_j_k_t\n",
        "      # rssi_j_k_car_id_t_1\n",
        "      # rssi_j_k_car_id_t\n",
        "\n",
        "      #########################################\n",
        "      #slice_id = list(j_k_list.keys())[idx]\n",
        "      #########################################\n",
        "\n",
        "      slice_id = action \n",
        "\n",
        "      target_mac_j_k = j_k_list[slice_id][0] \n",
        "      latency_server = j_k_list[slice_id][1]\n",
        "      load_j_k_t_1 = j_k_list[slice_id][2]\n",
        "      load_j_k_t = j_k_list[slice_id][3]\n",
        "      rssi_j_k_car_id_t_1 = j_k_list[slice_id][4]\n",
        "      rssi_j_k_car_id_t = j_k_list[slice_id][5]\n",
        "\n",
        "      #Mostrar datos del Slice\n",
        "      print(\"slice_id: \" + slice_id)\n",
        "      print(\"target_mac_j_k: \" + target_mac_j_k)\n",
        "      print(\"latency_server: \" + str(latency_server))\n",
        "      print(\"load_j_k_t_1: \" + str(load_j_k_t_1))\n",
        "      print(\"load_j_k_t: \" + str(load_j_k_t))\n",
        "      print(\"rssi_j_k_car_id_t_1: \" + str(rssi_j_k_car_id_t_1))\n",
        "      print(\"rssi_j_k_car_id_t: \" + str(rssi_j_k_car_id_t))\n",
        "\n",
        "      #=========================================\n",
        "      #== CALCULO DE RECOMPENSAS\n",
        "      print(\"=========================================\")\n",
        "      print(\"RECOMPENSAS:\")\n",
        "      print(\"\")\n",
        "\n",
        "      #== CALCULAR VARIACIÓN DE CARROS\n",
        "      E = 0.5\n",
        "      slice_variacionCarros_j_k = load_j_k_t - load_j_k_t_1\n",
        "      print(\"VARIACION DE CARROS: \" + str(slice_variacionCarros_j_k))\n",
        "\n",
        "      #== CALCULAR LATENCIA DE TRASPASO (HandoverLatency)\n",
        "      C = 0.5\n",
        "      HandoverLatency_Car = 1\n",
        "      print(\"HANDOVER LATENCY: \" + str(HandoverLatency_Car))\n",
        "      \n",
        "      \n",
        "\n",
        "      #== CALCULAR RECOMPENSA TOTAL\n",
        "      self.reward = E * slice_variacionCarros_j_k - C * HandoverLatency_Car\n",
        "      print(\"\")\n",
        "      print(\"RECOMPENSA TOTAL: \" + str(self.reward))\n",
        "      #=========================================\n",
        "\n",
        "      #Obtener nuevo estado según la acción\n",
        "      self.state = action\n",
        "      \n",
        "      #Mostrar estado nuevo\n",
        "      print(\"=========================================\")\n",
        "      print(\"NUEVO ESTADO:\" + self.state)\n",
        "      print(\"\")\n",
        "\n",
        "      print(\"=========================================\")\n",
        "    \n",
        "    else:\n",
        "      print(\"=========================================\")\n",
        "      print(\"Acción no escuchada\")\n",
        "      print(\"=========================================\")\n",
        "\n",
        "\n",
        "    return self.state, self.reward\n",
        "\n",
        "\n",
        "\n",
        "#print(\"== SDNController OK! ==\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Ym6SCR9LW8Do"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}